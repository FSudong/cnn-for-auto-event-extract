{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "    中文事件提取 卷积神经网络模型\n",
    "    john.zhang 2016-12-15\n",
    "\"\"\"\n",
    "class ace_cnn_model():\n",
    "    \"\"\"\n",
    "    中文事件的发现 由以后候选词和一个触发词是否构成事件 这里是另外一种比较简单的模型\n",
    "    Relation Classification via Convolutional Deep Neural Network\n",
    "    这篇文章句子特征用的是每一个词的上下文(包含当前词)作为整体的输入\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentence_length=30, event_num_labels=10, role_num_labels=6,windows=3, vocab_size=2048, word_embedding_size=100,\n",
    "                 pos_embedding_size=10, filter_sizes=[3, 4, 5], filter_num=50, batch_size=2):\n",
    "        \"\"\"\n",
    "        :param sentence_length: 输入句子的长度\n",
    "        :param event_num_labels: 事件类别数目\n",
    "        :param role_num_labels: 角色类别数目\n",
    "        :param windows: 窗口的大小\n",
    "        :param vocab_size: 训练集中词的数目\n",
    "        :param word_embedding_size: 词嵌入维数\n",
    "        :param pos_embedding_size: 位置嵌入维数\n",
    "        :param trigger_vec: 触发词向量\n",
    "        :param candidate_vec: 候选词向量\n",
    "        :param filter_sizes: 滤波器尺寸\n",
    "        :param filter_num: 滤波器数目\n",
    "        \"\"\"\n",
    "        input_x = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name=\"input_x\")\n",
    "        self.input_x = input_x\n",
    "        # 输入事件标签占位符\n",
    "        # [batch_size, event_num_labels]\n",
    "        input_event_y = tf.placeholder(tf.float32, shape=[batch_size, event_num_labels], name=\"input_event_y\")\n",
    "        self.input_event_y = input_event_y\n",
    "        # 输入角色标签占位符\n",
    "        # [batch_size, role_num_labels]\n",
    "        input_role_y = tf.placeholder(tf.float32, shape=[batch_size, role_num_labels], name=\"input_role_y\")\n",
    "        self.input_role_y = input_role_y\n",
    "        \n",
    "        # 触发词位置向量\n",
    "        input_t_pos = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name=\"input_t_pos\")\n",
    "        self.input_t_pos = input_t_pos\n",
    "        # 候选词位置向量\n",
    "        input_c_pos = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name=\"input_c_pos\")\n",
    "        self.input_c_pos = input_c_pos\n",
    "\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        \n",
    "        # 候选词的上下文组成的特征向量\n",
    "        input_c_context = tf.placeholder(tf.int32, shape=[batch_size, windows], name=\"input_c_context\")\n",
    "        self.input_c_context = input_c_context\n",
    "        # 　触发词的上下文组成的特征向量\n",
    "        input_t_context = tf.placeholder(tf.int32, shape=[batch_size, windows], name=\"input_t_context\")\n",
    "        self.input_t_context = input_t_context\n",
    "        # 生成word_embedding 这部分操作相对比较简单 建议用cpu做\n",
    "        with tf.device('/gpu:0'), tf.name_scope(\"word_embedding_layer\"):\n",
    "            # 词表 [vocab_size, embedding_size]\n",
    "            W = tf.Variable(tf.random_normal(shape=[vocab_size, word_embedding_size], mean=0.0, stddev=0.5),\n",
    "                            name=\"word_table\")\n",
    "            # 句子特征向量 [batch_size, sentence_length, word_embedding]\n",
    "            input_word_vec = tf.nn.embedding_lookup(W, input_x)            \n",
    "            #             print sentence_features\n",
    "            #             print sentences_features_vec\n",
    "            #             根据 候选词位置 触发词位置 选取位置向量\n",
    "            #             在(-sentence_length+1,sentence_length-1)之间一共2*(sentence_length-1)+1个数\n",
    "            #             look_up 变成合适的正整数\n",
    "            input_t_pos_t = input_t_pos + (sentence_length - 1)\n",
    "            Tri_pos = tf.Variable(\n",
    "                tf.random_normal(shape=[2 * (sentence_length - 1) + 1, pos_embedding_size], mean=0.0, stddev=0.5),\n",
    "                name=\"tri_pos_table\")\n",
    "            input_t_pos_vec = tf.nn.embedding_lookup(Tri_pos, input_t_pos_t)\n",
    "            #             look_up 变成合适的正整数\n",
    "            input_c_pos_c = input_c_pos + (sentence_length - 1)\n",
    "            Can_pos = tf.Variable(\n",
    "                tf.random_normal(shape=[2 * (sentence_length - 1) + 1, pos_embedding_size], mean=0.0, stddev=0.5),\n",
    "                name=\"candidate_pos_table\")\n",
    "            input_c_pos_vec = tf.nn.embedding_lookup(Can_pos, input_c_pos_c)\n",
    "            #             print input_t_pos_vec\n",
    "            #             print input_c_pos_vec\n",
    "            # 将距离特征和句子的词特征构成一个整理的特征 作为卷积神经网络的输入\n",
    "            # [batch_size, sentence_length, word_embedding_size+2*pos_size]\n",
    "            input_sentence_vec = tf.concat(2, [input_word_vec, input_t_pos_vec, input_c_pos_vec])\n",
    "            # CNN支持4d输入 因此增加一维向量 用于表示输入通道数目\n",
    "            intput_sentence_vec_expanded = tf.expand_dims(input_sentence_vec, -1)\n",
    "            # 词汇特征 lexical leval features\n",
    "            # 候选词极其上下文 [batch_size, windows, word_embedding_size]\n",
    "            input_c_context_vec = tf.nn.embedding_lookup(W, input_c_context)\n",
    "            # 触发词及其上下文 [batch_size, windows, word_embedding_size]\n",
    "            input_t_context_vec = tf.nn.embedding_lookup(W, input_t_context)\n",
    "            # print input_sentence_vec\n",
    "            # print input_c_context_vec\n",
    "            # print input_t_context_vec\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.device('/gpu:0'), tf.name_scope('conv-maxpool-%s' % filter_size):\n",
    "                # 这里的句子特征考虑的上当前词\n",
    "                filter_shape = [filter_size,word_embedding_size + 2 * pos_embedding_size, 1, filter_num]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"b\")\n",
    "                # 卷积运算\n",
    "                conv = tf.nn.conv2d(\n",
    "                    intput_sentence_vec_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # 最大化池化 暂时不用动态池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sentence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "        # 使用到的所有滤波器数目(输出的通道数目)\n",
    "        num_filters_total = filter_num * len(filter_sizes)\n",
    "        # 多通道的数据合并\n",
    "        h_pool = tf.concat(3, pooled_outputs)\n",
    "        #         print sentences_features_vec_flat\n",
    "        #         print h_pool\n",
    "        # 展开送入下一层分类器\n",
    "        # [batch_size, num_filters_total]\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        # print h_pool_flat\n",
    "        # 候选词极其上下文 触发词极其上下文 [batch_size, windows, word_embedding_size] => [batch_size, windows*word_embedding_size]\n",
    "        # [batch_size, windows * word_embedding_size]\n",
    "        input_c_context_vec_flat = tf.reshape(input_c_context_vec, [-1, windows * word_embedding_size])\n",
    "        # print input_c_context_vec_flat\n",
    "        # [batch_size, windows * word_embedding_size]\n",
    "        input_t_context_vec_flat = tf.reshape(input_t_context_vec, [-1, windows * word_embedding_size])\n",
    "        # print input_t_context_vec_flat\n",
    "        # print h_pool_flat\n",
    "        # 原本的论文将lexical leval features和sentence leval features组合\n",
    "        # 这里采用multi-task的思路 sentence leval features主要应用于事件的发现\n",
    "        # lexical leval features主要应用于角色的分类\n",
    "        input_sentence_features = h_pool_flat\n",
    "        input_lexical_features = tf.concat(1, [input_c_context_vec_flat, input_t_context_vec_flat])\n",
    "        # 总体的分类器 经过一层dropout 然后再送入softmax\n",
    "        with tf.device('/gpu:0'), tf.name_scope('dropout'):\n",
    "            input_sentence_features_dropout = tf.nn.dropout(input_sentence_features, dropout_keep_prob)   \n",
    "            input_lexical_features_dropout = tf.nn.dropout(input_lexical_features, dropout_keep_prob)\n",
    "        # 分类器\n",
    "        with tf.device('/gpu:0'), tf.name_scope('softmax'):\n",
    "            # num_filters_total是卷积之后的结果 sentence level features\n",
    "            # 事件类别分类\n",
    "            W1 = tf.Variable(tf.truncated_normal([num_filters_total, event_num_labels], stddev=0.1), name=\"W1\")\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[event_num_labels]), name=\"b1\")\n",
    "            xw1 = tf.nn.xw_plus_b(input_sentence_features_dropout, W1, b1)\n",
    "            scores_event = tf.nn.softmax(xw1, name=\"scores_event\")\n",
    "            # 2*windows*word_embedding_size是lexical leval features 具体的请看论文当中的详细介绍\n",
    "            predicts_event = tf.arg_max(scores_event, dimension=1, name=\"predicts_event\")\n",
    "            self.scores_event = scores_event\n",
    "            self.predicts_event = predicts_event\n",
    "            # 角色类别分类\n",
    "            W2 = tf.Variable(tf.truncated_normal([num_filters_total+2*windows*word_embedding_size+event_num_labels, role_num_labels], stddev=0.1), name=\"W2\")\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[role_num_labels]), name=\"b2\")\n",
    "             # 将句子特征 候选词和词性以及触发词和词性特征合并用于角色分类\n",
    "            # input_sentence_features_dropout  sentence features\n",
    "            # input_lexical_features_dropout  lexical_features\n",
    "            # scores_event  事件类型分类的输出得分结果\n",
    "            all_input_fatures = tf.concat(1, [input_lexical_features_dropout, input_sentence_features, scores_event])\n",
    "            xw2 = tf.nn.xw_plus_b(all_input_fatures, W2, b2)\n",
    "            scores_role = tf.nn.softmax(xw2, name=\"scores_role\")\n",
    "            predicts_role = tf.arg_max(scores_role, dimension=1, name=\"predicts_role\")\n",
    "            self.scores_role = scores_role\n",
    "            self.predicts_role = predicts_role\n",
    "        # 模型的代价函数 交叉熵代价函数\n",
    "        with tf.device('/gpu:0'), tf.name_scope('loss'):\n",
    "            # 事件的交叉熵代价函数\n",
    "            entropy_event = tf.nn.softmax_cross_entropy_with_logits(scores_event, input_event_y)\n",
    "            loss_event = tf.reduce_mean(entropy_event)\n",
    "            self.loss_event = loss_event\n",
    "            # 角色的交叉熵代价函数\n",
    "            entropy_role = tf.nn.softmax_cross_entropy_with_logits(scores_role, input_role_y)\n",
    "            loss_role = tf.reduce_mean(entropy_role)\n",
    "            self.loss_role = loss_role\n",
    "        # 准确度 用于每一次训练时调用\n",
    "        with tf.device('/gpu:0'), tf.name_scope(\"accuracy\"):\n",
    "            # 事件的准确度\n",
    "            correct_event = tf.equal(predicts_event, tf.argmax(input_event_y, 1))\n",
    "            accuracy_event = tf.reduce_mean(tf.cast(correct_event, \"float\"), name=\"accuracy_event\")\n",
    "            self.accuracy_event = accuracy_event\n",
    "            # 角色的准确度\n",
    "            correct_role = tf.equal(predicts_role, tf.argmax(input_role_y, 1))\n",
    "            accuracy_role = tf.reduce_mean(tf.cast(correct_role, \"float\"), name=\"accuracy_role\")\n",
    "            self.accuracy_role = accuracy_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/njit/Deeplearning/John_toturial/ace/ace_cnn_model_02/1482228822\n",
      "\n",
      "stype:event, 0: , loss 2.26838, acc 0\n",
      "stype:role, 0: , loss 1.92841, acc 0\n",
      "stype:event, 1: , loss 2.25971, acc 0.05\n",
      "stype:role, 1: , loss 1.84563, acc 0.2\n",
      "stype:event, 2: , loss 2.06466, acc 0.35\n",
      "stype:role, 2: , loss 1.8259, acc 0.1\n",
      "stype:event, 3: , loss 1.85558, acc 0.6\n",
      "stype:role, 3: , loss 1.59052, acc 0.5\n",
      "stype:event, 4: , loss 2.20019, acc 0.15\n",
      "stype:role, 4: , loss 1.45519, acc 0.65\n",
      "stype:event, 5: , loss 2.11449, acc 0.25\n",
      "stype:role, 5: , loss 1.5944, acc 0.45\n",
      "stype:event, 6: , loss 2.14568, acc 0.2\n",
      "stype:role, 6: , loss 1.37812, acc 0.65\n",
      "stype:event, 7: , loss 1.92274, acc 0.45\n",
      "stype:role, 7: , loss 1.49343, acc 0.55\n",
      "stype:event, 8: , loss 2.07298, acc 0.3\n",
      "stype:role, 8: , loss 1.48839, acc 0.55\n",
      "stype:event, 9: , loss 1.96061, acc 0.4\n",
      "stype:role, 9: , loss 1.28431, acc 0.75\n",
      "stype:event, 10: , loss 1.96186, acc 0.35\n",
      "stype:role, 10: , loss 1.44246, acc 0.6\n",
      "stype:event, 11: , loss 2.05776, acc 0.3\n",
      "stype:role, 11: , loss 1.34364, acc 0.7\n",
      "stype:event, 12: , loss 2.21892, acc 0.1\n",
      "stype:role, 12: , loss 1.44322, acc 0.6\n",
      "stype:event, 13: , loss 2.00526, acc 0.35\n",
      "stype:role, 13: , loss 1.44371, acc 0.6\n",
      "stype:event, 14: , loss 2.04607, acc 0.3\n",
      "stype:role, 14: , loss 1.42028, acc 0.6\n",
      "stype:event, 15: , loss 2.12042, acc 0.2\n",
      "stype:role, 15: , loss 1.54171, acc 0.5\n",
      "stype:event, 16: , loss 1.80304, acc 0.6\n",
      "stype:role, 16: , loss 1.3437, acc 0.7\n",
      "stype:event, 17: , loss 2.1258, acc 0.2\n",
      "stype:role, 17: , loss 1.59079, acc 0.45\n",
      "stype:event, 18: , loss 1.86074, acc 0.5\n",
      "stype:role, 18: , loss 1.54332, acc 0.5\n",
      "stype:event, 19: , loss 2.23258, acc 0.1\n",
      "stype:role, 19: , loss 1.39559, acc 0.65\n",
      "stype:event, 20: , loss 1.88666, acc 0.55\n",
      "stype:role, 20: , loss 1.49068, acc 0.55\n",
      "stype:event, 21: , loss 2.03029, acc 0.35\n",
      "stype:role, 21: , loss 1.39289, acc 0.65\n",
      "stype:event, 22: , loss 2.02451, acc 0.3\n",
      "stype:role, 22: , loss 1.3364, acc 0.7\n",
      "stype:event, 23: , loss 1.99964, acc 0.35\n",
      "stype:role, 23: , loss 1.29192, acc 0.75\n",
      "stype:event, 24: , loss 2.04323, acc 0.35\n",
      "stype:role, 24: , loss 1.24433, acc 0.8\n",
      "stype:event, 25: , loss 1.90787, acc 0.5\n",
      "stype:role, 25: , loss 1.52844, acc 0.5\n",
      "stype:event, 26: , loss 1.94937, acc 0.4\n",
      "stype:role, 26: , loss 1.36501, acc 0.65\n",
      "stype:event, 27: , loss 1.85449, acc 0.5\n",
      "stype:role, 27: , loss 1.22666, acc 0.8\n",
      "stype:event, 28: , loss 1.89303, acc 0.45\n",
      "stype:role, 28: , loss 1.35408, acc 0.7\n",
      "stype:event, 29: , loss 1.9811, acc 0.35\n",
      "stype:role, 29: , loss 1.49127, acc 0.55\n",
      "stype:event, 30: , loss 1.82508, acc 0.6\n",
      "stype:role, 30: , loss 1.46685, acc 0.55\n",
      "stype:event, 31: , loss 2.0338, acc 0.3\n",
      "stype:role, 31: , loss 1.41382, acc 0.6\n",
      "stype:event, 32: , loss 2.10892, acc 0.25\n",
      "stype:role, 32: , loss 1.5597, acc 0.5\n",
      "stype:event, 33: , loss 1.7524, acc 0.65\n",
      "stype:role, 33: , loss 1.62745, acc 0.45\n",
      "stype:event, 34: , loss 1.93436, acc 0.45\n",
      "stype:role, 34: , loss 1.51331, acc 0.5\n",
      "stype:event, 35: , loss 1.95479, acc 0.45\n",
      "stype:role, 35: , loss 1.42811, acc 0.65\n",
      "stype:event, 36: , loss 1.82098, acc 0.55\n",
      "stype:role, 36: , loss 1.45409, acc 0.6\n",
      "stype:event, 37: , loss 1.84474, acc 0.55\n",
      "stype:role, 37: , loss 1.37367, acc 0.7\n",
      "stype:event, 38: , loss 1.8201, acc 0.5\n",
      "stype:role, 38: , loss 1.45876, acc 0.6\n",
      "stype:event, 39: , loss 1.92066, acc 0.5\n",
      "stype:role, 39: , loss 1.60636, acc 0.45\n",
      "stype:event, 40: , loss 1.92163, acc 0.4\n",
      "stype:role, 40: , loss 1.3961, acc 0.65\n",
      "stype:event, 41: , loss 1.91802, acc 0.45\n",
      "stype:role, 41: , loss 1.56378, acc 0.5\n",
      "stype:event, 42: , loss 1.9112, acc 0.45\n",
      "stype:role, 42: , loss 1.39253, acc 0.65\n",
      "stype:event, 43: , loss 1.98771, acc 0.4\n",
      "stype:role, 43: , loss 1.29464, acc 0.75\n",
      "stype:event, 44: , loss 1.79331, acc 0.55\n",
      "stype:role, 44: , loss 1.59516, acc 0.45\n",
      "stype:event, 45: , loss 1.9462, acc 0.4\n",
      "stype:role, 45: , loss 1.41711, acc 0.6\n",
      "stype:event, 46: , loss 2.03765, acc 0.3\n",
      "stype:role, 46: , loss 1.49379, acc 0.55\n",
      "stype:event, 47: , loss 1.7869, acc 0.6\n",
      "stype:role, 47: , loss 1.29289, acc 0.75\n",
      "stype:event, 48: , loss 1.75392, acc 0.55\n",
      "stype:role, 48: , loss 1.43896, acc 0.6\n",
      "stype:event, 49: , loss 1.90448, acc 0.45\n",
      "stype:role, 49: , loss 1.35532, acc 0.7\n",
      "stype:event, 50: , loss 1.98272, acc 0.35\n",
      "stype:role, 50: , loss 1.29673, acc 0.75\n",
      "stype:event, 51: , loss 1.65788, acc 0.7\n",
      "stype:role, 51: , loss 1.64108, acc 0.4\n",
      "stype:event, 52: , loss 2.09951, acc 0.25\n",
      "stype:role, 52: , loss 1.45837, acc 0.6\n",
      "stype:event, 53: , loss 1.86238, acc 0.5\n",
      "stype:role, 53: , loss 1.31298, acc 0.7\n",
      "stype:event, 54: , loss 1.74997, acc 0.65\n",
      "stype:role, 54: , loss 1.29799, acc 0.75\n",
      "stype:event, 55: , loss 1.85289, acc 0.55\n",
      "stype:role, 55: , loss 1.27125, acc 0.75\n",
      "stype:event, 56: , loss 1.88104, acc 0.45\n",
      "stype:role, 56: , loss 1.29119, acc 0.75\n",
      "stype:event, 57: , loss 1.71634, acc 0.65\n",
      "stype:role, 57: , loss 1.49726, acc 0.55\n",
      "stype:event, 58: , loss 1.81278, acc 0.55\n",
      "stype:role, 58: , loss 1.53722, acc 0.5\n",
      "stype:event, 59: , loss 1.80996, acc 0.55\n",
      "stype:role, 59: , loss 1.35923, acc 0.7\n",
      "stype:event, 60: , loss 1.70187, acc 0.7\n",
      "stype:role, 60: , loss 1.50599, acc 0.55\n",
      "stype:event, 61: , loss 1.83618, acc 0.55\n",
      "stype:role, 61: , loss 1.34306, acc 0.7\n",
      "stype:event, 62: , loss 1.65931, acc 0.7\n",
      "stype:role, 62: , loss 1.34828, acc 0.7\n",
      "stype:event, 63: , loss 1.71093, acc 0.65\n",
      "stype:role, 63: , loss 1.43476, acc 0.6\n",
      "stype:event, 64: , loss 1.56212, acc 0.8\n",
      "stype:role, 64: , loss 1.44115, acc 0.6\n",
      "stype:event, 65: , loss 1.85814, acc 0.5\n",
      "stype:role, 65: , loss 1.53915, acc 0.5\n",
      "stype:event, 66: , loss 1.84148, acc 0.55\n",
      "stype:role, 66: , loss 1.27075, acc 0.75\n",
      "stype:event, 67: , loss 1.74176, acc 0.6\n",
      "stype:role, 67: , loss 1.18498, acc 0.85\n",
      "stype:event, 68: , loss 1.86499, acc 0.55\n",
      "stype:role, 68: , loss 1.46021, acc 0.6\n",
      "stype:event, 69: , loss 1.67956, acc 0.7\n",
      "stype:role, 69: , loss 1.23457, acc 0.75\n",
      "stype:event, 70: , loss 1.9476, acc 0.45\n",
      "stype:role, 70: , loss 1.36676, acc 0.7\n",
      "stype:event, 71: , loss 1.69716, acc 0.7\n",
      "stype:role, 71: , loss 1.21089, acc 0.85\n",
      "stype:event, 72: , loss 1.71362, acc 0.65\n",
      "stype:role, 72: , loss 1.39177, acc 0.65\n",
      "stype:event, 73: , loss 1.70086, acc 0.65\n",
      "stype:role, 73: , loss 1.4792, acc 0.55\n",
      "stype:event, 74: , loss 1.68582, acc 0.7\n",
      "stype:role, 74: , loss 1.4183, acc 0.6\n",
      "stype:event, 75: , loss 1.75926, acc 0.65\n",
      "stype:role, 75: , loss 1.40028, acc 0.65\n",
      "stype:event, 76: , loss 2.00466, acc 0.35\n",
      "stype:role, 76: , loss 1.34587, acc 0.7\n",
      "stype:event, 77: , loss 1.76203, acc 0.6\n",
      "stype:role, 77: , loss 1.50075, acc 0.5\n",
      "stype:event, 78: , loss 1.55962, acc 0.8\n",
      "stype:role, 78: , loss 1.2983, acc 0.75\n",
      "stype:event, 79: , loss 1.70156, acc 0.65\n",
      "stype:role, 79: , loss 1.56798, acc 0.45\n",
      "stype:event, 80: , loss 1.77155, acc 0.6\n",
      "stype:role, 80: , loss 1.40919, acc 0.65\n",
      "stype:event, 81: , loss 1.83557, acc 0.5\n",
      "stype:role, 81: , loss 1.4151, acc 0.65\n",
      "stype:event, 82: , loss 1.68611, acc 0.7\n",
      "stype:role, 82: , loss 1.25945, acc 0.8\n",
      "stype:event, 83: , loss 1.74313, acc 0.7\n",
      "stype:role, 83: , loss 1.3931, acc 0.65\n",
      "stype:event, 84: , loss 1.68359, acc 0.7\n",
      "stype:role, 84: , loss 1.42605, acc 0.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bde9f1370f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mdata_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_cnn_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;31m#                 print \", \".join(map(lambda t:datas.all_words[t]  , x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m#     事件类型预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/njit/Deeplearning/John_toturial/ace/DataSets_multi_task.py\u001b[0m in \u001b[0;36mnext_cnn_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# x 就是词向量组成的句子特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0msentence_fatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_context_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepos_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0;31m# 由每一个句子当中的词的上下文组成的词组成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0msentences_fatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_fatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/njit/Deeplearning/John_toturial/ace/DataSets_multi_task.py\u001b[0m in \u001b[0;36mget_context_features\u001b[0;34m(x, epos_id, windows)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# 上下文长度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mx_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mx_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mepos_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mx_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_pad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mepos_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# 依据x_pad生成多少个context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "测试程序 john.zhang 2016-11-28 已验证 程序可以运行\n",
    "\"\"\"\n",
    "# 数据集配置参数\n",
    "from DataSets_multi_task import datasets\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "# 文件信息\n",
    "file = 'datas_ace.txt'\n",
    "# 生成文件的存储位置\n",
    "store_path = \"ace_data_2016_12_02\"\n",
    "# batch_size的大小\n",
    "data_batch_size = 20\n",
    "# 句子的最大长度\n",
    "max_sequence_length = 25\n",
    "# 选取的上下文窗口的大小\n",
    "windows = 3\n",
    "# 数据集\n",
    "datas = datasets(file=file, store_path=store_path, batch_size=data_batch_size, max_sequence_length=max_sequence_length,\n",
    "                 windows=windows)\n",
    "# 神经网络模型的一些参数\n",
    "# 模型的最大长度\n",
    "sentence_length = max_sequence_length\n",
    "event_num_labels = datas.labels_event_size  # 事件类别数目\n",
    "role_num_labels = datas.labels_role_size  # 角色类别数目\n",
    "vocab_size = datas.words_size  # 训练集中词的数目\n",
    "word_embedding_size = 100  # 词嵌入维数\n",
    "pos_embedding_size = 10  # 位置嵌入维数\n",
    "filter_sizes = [3, 4, 5]  # 滤波器大小\n",
    "filter_num = 100  # 滤波器大小\n",
    "batch_size = None  # tensorflow中支持为定义长度的标记符合\n",
    "lr = 1e-3  # 学习率\n",
    "num_epochs = 1000\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        # 模型文件\n",
    "        model = ace_cnn_model(sentence_length=sentence_length, \n",
    "                              event_num_labels=event_num_labels, \n",
    "                              role_num_labels=role_num_labels,\n",
    "                              vocab_size=vocab_size,\n",
    "                              word_embedding_size=word_embedding_size,\n",
    "                              pos_embedding_size=pos_embedding_size, \n",
    "                              filter_sizes=filter_sizes, \n",
    "                              filter_num=filter_num,\n",
    "                              batch_size=batch_size)\n",
    "        # 模型优化算法  两种方式：\n",
    "        # 方式一 采用两种优化器分别进行\n",
    "        # 方式二 将两种误差函数合并一起进行\n",
    "        optimizer_event = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars_event = optimizer_event.compute_gradients(model.loss_event) # 事件优化器\n",
    "        train_op_event = optimizer_event.apply_gradients(grads_and_vars_event)\n",
    "        \n",
    "        optimizer_role = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars_role = optimizer_role.compute_gradients(model.loss_role)  # 角色优化器\n",
    "        train_op_role = optimizer_role.apply_gradients(grads_and_vars_role)\n",
    "\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"ace_cnn_model_02\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables(), max_to_keep=100)  # 最大支持存储100个模型\n",
    "        # 初始化　变量\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        def train_step(input_x, input_event_y, input_role_y, input_t, input_c, input_t_pos, input_c_pos, dropout_keep_prob\n",
    "                       ,sentence_features,input_t_context,input_c_context,train_op,loss,accuracy,epoch, stype):\n",
    "            feed_dict = {\n",
    "                model.input_x: input_x,  # 句子级别特征\n",
    "                model.input_event_y: input_event_y,  # 事件类别\n",
    "                model.input_role_y: input_role_y,  # 角色类别\n",
    "                # model.input_t:input_t, # 触发词\n",
    "                # model.input_c:input_c, # 候选词\n",
    "                model.input_t_pos: input_t_pos, # 触发词位置向量\n",
    "                model.input_c_pos: input_c_pos, # 候选词位置向量\n",
    "                model.dropout_keep_prob: dropout_keep_prob, # drop_out 概率\n",
    "#                 model.sentence_features : sentence_features, # 句子级别的特征 这里每一个词都有其上下文组成\n",
    "                model.input_t_context:input_t_context, # 触发词以及其上下文\n",
    "                model.input_c_context:input_c_context  # 候选词以及上下文\n",
    "            }\n",
    "            _, loss, accuracy = sess.run(\n",
    "                [train_op, loss, accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"stype:{}, {}: , loss {:g}, acc {:g}\".format(stype, epoch, loss, accuracy))\n",
    "\n",
    "\n",
    "        # 测试阶段不需要计算梯度 也不需要进行权值更新 仅仅需要计算acc的值\n",
    "        def eval_step(input_x, input_event_y, input_role_y, input_t, input_c, input_t_pos, input_c_pos, dropout_keep_prob\n",
    "                      , sentence_features, input_t_context, input_c_context,accuracy, predicts, stype):\n",
    "            feed_dict = {\n",
    "                model.input_x: input_x, # 句子级别特征\n",
    "                model.input_event_y: input_event_y, # 事件类别\n",
    "                model.input_role_y: input_role_y, # 角色类别\n",
    "                # model.input_t:input_t,  # 触发词\n",
    "                # model.input_c:input_c,  # 候选词\n",
    "                model.input_t_pos: input_t_pos,   # 触发词位置向量\n",
    "                model.input_c_pos: input_c_pos,   # 候选词位置向量\n",
    "                model.dropout_keep_prob: dropout_keep_prob,  # drop_out 概率\n",
    "#                 model.sentence_features: sentence_features,  # 句子级别的特征 这里每一个词都有其上下文组成\n",
    "                model.input_t_context: input_t_context,  # 触发词以及其上下文\n",
    "                model.input_c_context: input_c_context   # 候选词以及上下文\n",
    "            }\n",
    "            accuracy, predicts = sess.run([accuracy, predicts], feed_dict) \n",
    "            print (\"{} eval accuracy:{}\".format(stype, accuracy))\n",
    "            return predicts\n",
    "\n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            for j in range(datas.instances_size // data_batch_size):\n",
    "                x, t, c, y_e, y_r, pos_c, pos_t, sentences_f, c_context, t_context, _ = datas.next_cnn_data()\n",
    "#                 print \", \".join(map(lambda t:datas.all_words[t]  , x[0])) \n",
    "            #     事件类型预测 \n",
    "            train_step(input_x=x, input_event_y=y_e, input_role_y=y_r,\n",
    "                           input_t=t, input_c=c, input_c_pos=pos_c, input_t_pos=pos_t,\n",
    "                           dropout_keep_prob=0.8, sentence_features=sentences_f, \n",
    "                           input_t_context= t_context,input_c_context = c_context,\n",
    "                           train_op = train_op_event, loss=model.loss_event,accuracy=model.accuracy_event,\n",
    "                           epoch = i, stype=\"event\"\n",
    "                          )\n",
    "            #     角色类型预测\n",
    "            train_step(input_x=x, input_event_y=y_e, input_role_y=y_r,\n",
    "                           input_t=t, input_c=c, input_c_pos=pos_c, input_t_pos=pos_t,\n",
    "                           dropout_keep_prob=0.8, sentence_features=sentences_f, \n",
    "                           input_t_context= t_context,input_c_context = c_context,\n",
    "                           train_op = train_op_role, loss=model.loss_role,accuracy=model.accuracy_role,\n",
    "                           epoch = i, stype=\"role\"\n",
    "                          )\n",
    "        # john.zhang 2016-12-16 最后50个instance作为测试集 用于测试数据\n",
    "        print \"----------------------------华丽的分割线-----------------------------------------\"\n",
    "        x, t, c, y_e, y_r, pos_c, pos_t, sentences_f, c_context, t_context, _ = datas.eval_cnn_data()\n",
    "        # 事件类型预测\n",
    "        predicts_event = eval_step(input_x=x, input_event_y=y_e, input_role_y=y_r, input_t=t, input_c=c, input_c_pos=pos_c, input_t_pos=pos_t,\n",
    "                  dropout_keep_prob=1.0, sentence_features=sentences_f, input_t_context=t_context,\n",
    "                  input_c_context=c_context, accuracy=model.accuracy_event, predicts=model.predicts_event, stype=\"event\")\n",
    "        # 角色类型预测\n",
    "        predicts_role = eval_step(input_x=x, input_event_y=y_e, input_role_y=y_r, input_t=t, input_c=c, input_c_pos=pos_c, input_t_pos=pos_t,\n",
    "                  dropout_keep_prob=1.0, sentence_features=sentences_f, input_t_context=t_context,\n",
    "                  input_c_context=c_context, accuracy=model.accuracy_role, predicts=model.predicts_role, stype=\"role\")\n",
    "        convert_event = {0:\"非事件\", 1:\"股票增长类事件\", 2:\"股票降低类事件\", 3:\"股票异动类事件\", 4:\"股票交易类事件\", \n",
    "                        5:\"业绩上升类事件\",6:\"业绩下滑类事件\",7:\"产品涨价类事件\",8:\"产品跌价类事件\"\n",
    "                        }    # 事件类型准换\n",
    "        convert_role = {0:\"其它角色\", 1:\"施事角色\", 2:\"受事角色\", 3:\"时间角色\", 4:\"地点角色\", 5:\"数字角色\"}    # 角色类型转换\n",
    "        # 输出测试结果\n",
    "        for i in range(len(x)):\n",
    "            print \"输入数据：{}\".format(\", \".join(map(lambda h: datas.all_words[h], x[i])))\n",
    "            print \"触发词：{}\".format(\", \".join(map(lambda h: datas.all_words[h], t[i])))\n",
    "            print \"候选词：{}\".format(\", \".join(map(lambda h: datas.all_words[h], c[i])))\n",
    "            print \"预测事件类别:{}\".format(convert_event[predicts_event[i]])\n",
    "            print \"预测角色类别:{}\".format(convert_role[predicts_role[i]])\n",
    "#             print \"----------------------------华丽的分割线-----------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
