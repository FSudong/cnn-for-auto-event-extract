{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "    中文事件提取 卷积神经网络模型\n",
    "    john.zhang 2016-12-15\n",
    "\"\"\"\n",
    "class ace_cnn_model():\n",
    "    \"\"\"\n",
    "    中文事件的发现 由以后候选词和一个触发词是否构成事件 这里是另外一种比较简单的模型\n",
    "    Relation Classification via Convolutional Deep Neural Network\n",
    "    这篇文章句子特征用的是每一个词的上下文(包含当前词)作为整体的输入\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentence_length=30, event_num_labels=10, role_num_labels=6,windows=3, vocab_size=2048, word_embedding_size=100,\n",
    "                 pos_embedding_size=10, filter_sizes=[3, 4, 5], filter_num=50, batch_size=2):\n",
    "        \"\"\"\n",
    "        :param sentence_length: 输入句子的长度\n",
    "        :param event_num_labels: 事件类别数目\n",
    "        :param role_num_labels: 角色类别数目\n",
    "        :param windows: 窗口的大小\n",
    "        :param vocab_size: 训练集中词的数目\n",
    "        :param word_embedding_size: 词嵌入维数\n",
    "        :param pos_embedding_size: 位置嵌入维数\n",
    "        :param trigger_vec: 触发词向量\n",
    "        :param candidate_vec: 候选词向量\n",
    "        :param filter_sizes: 滤波器尺寸\n",
    "        :param filter_num: 滤波器数目\n",
    "        \"\"\"\n",
    "        input_x = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name=\"input_x\")\n",
    "        self.input_x = input_x\n",
    "        # 输入事件标签占位符\n",
    "        # [batch_size, event_num_labels]\n",
    "        input_event_y = tf.placeholder(tf.float32, shape=[batch_size, event_num_labels], name=\"input_event_y\")\n",
    "        self.input_event_y = input_event_y\n",
    "        # 输入角色标签占位符\n",
    "        # [batch_size, role_num_labels]\n",
    "        input_role_y = tf.placeholder(tf.float32, shape=[batch_size, role_num_labels], name=\"input_role_y\")\n",
    "        self.input_role_y = input_role_y\n",
    "        \n",
    "        # 触发词位置向量\n",
    "        input_t_pos = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name=\"input_t_pos\")\n",
    "        self.input_t_pos = input_t_pos\n",
    "        # 候选词位置向量\n",
    "        input_c_pos = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name=\"input_c_pos\")\n",
    "        self.input_c_pos = input_c_pos\n",
    "\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        \n",
    "        # 候选词的上下文组成的特征向量\n",
    "        input_c_context = tf.placeholder(tf.int32, shape=[batch_size, windows], name=\"input_c_context\")\n",
    "        self.input_c_context = input_c_context\n",
    "        # 　触发词的上下文组成的特征向量\n",
    "        input_t_context = tf.placeholder(tf.int32, shape=[batch_size, windows], name=\"input_t_context\")\n",
    "        self.input_t_context = input_t_context\n",
    "        # 生成word_embedding 这部分操作相对比较简单 建议用cpu做\n",
    "        with tf.device('/gpu:0'), tf.name_scope(\"word_embedding_layer\"):\n",
    "            # 词表 [vocab_size, embedding_size]\n",
    "            W = tf.Variable(tf.random_normal(shape=[vocab_size, word_embedding_size], mean=0.0, stddev=0.5),\n",
    "                            name=\"word_table\")\n",
    "            # 句子特征向量 [batch_size, sentence_length, word_embedding]\n",
    "            input_word_vec = tf.nn.embedding_lookup(W, input_x)            \n",
    "            #             print sentence_features\n",
    "            #             print sentences_features_vec\n",
    "            #             根据 候选词位置 触发词位置 选取位置向量\n",
    "            #             在(-sentence_length+1,sentence_length-1)之间一共2*(sentence_length-1)+1个数\n",
    "            #             look_up 变成合适的正整数\n",
    "            input_t_pos_t = input_t_pos + (sentence_length - 1)\n",
    "            Tri_pos = tf.Variable(\n",
    "                tf.random_normal(shape=[2 * (sentence_length - 1) + 1, pos_embedding_size], mean=0.0, stddev=0.5),\n",
    "                name=\"tri_pos_table\")\n",
    "            input_t_pos_vec = tf.nn.embedding_lookup(Tri_pos, input_t_pos_t)\n",
    "            #             look_up 变成合适的正整数\n",
    "            input_c_pos_c = input_c_pos + (sentence_length - 1)\n",
    "            Can_pos = tf.Variable(\n",
    "                tf.random_normal(shape=[2 * (sentence_length - 1) + 1, pos_embedding_size], mean=0.0, stddev=0.5),\n",
    "                name=\"candidate_pos_table\")\n",
    "            input_c_pos_vec = tf.nn.embedding_lookup(Can_pos, input_c_pos_c)\n",
    "            #             print input_t_pos_vec\n",
    "            #             print input_c_pos_vec\n",
    "            # 将距离特征和句子的词特征构成一个整理的特征 作为卷积神经网络的输入\n",
    "            # [batch_size, sentence_length, word_embedding_size+2*pos_size]\n",
    "            input_sentence_vec = tf.concat(2, [input_word_vec, input_t_pos_vec, input_c_pos_vec])\n",
    "            # CNN支持4d输入 因此增加一维向量 用于表示输入通道数目\n",
    "            intput_sentence_vec_expanded = tf.expand_dims(input_sentence_vec, -1)\n",
    "            # 词汇特征 lexical leval features\n",
    "            # 候选词极其上下文 [batch_size, windows, word_embedding_size]\n",
    "            input_c_context_vec = tf.nn.embedding_lookup(W, input_c_context)\n",
    "            # 触发词及其上下文 [batch_size, windows, word_embedding_size]\n",
    "            input_t_context_vec = tf.nn.embedding_lookup(W, input_t_context)\n",
    "            # print input_sentence_vec\n",
    "            # print input_c_context_vec\n",
    "            # print input_t_context_vec\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.device('/gpu:0'), tf.name_scope('conv-maxpool-%s' % filter_size):\n",
    "                # 这里的句子特征考虑的上当前词\n",
    "                filter_shape = [filter_size,word_embedding_size + 2 * pos_embedding_size, 1, filter_num]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"b\")\n",
    "                # 卷积运算\n",
    "                conv = tf.nn.conv2d(\n",
    "                    intput_sentence_vec_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # 最大化池化 暂时不用动态池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sentence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "        # 使用到的所有滤波器数目(输出的通道数目)\n",
    "        num_filters_total = filter_num * len(filter_sizes)\n",
    "        # 多通道的数据合并\n",
    "        h_pool = tf.concat(3, pooled_outputs)\n",
    "        #         print sentences_features_vec_flat\n",
    "        #         print h_pool\n",
    "        # 展开送入下一层分类器\n",
    "        # [batch_size, num_filters_total]\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        # print h_pool_flat\n",
    "        # 候选词极其上下文 触发词极其上下文 [batch_size, windows, word_embedding_size] => [batch_size, windows*word_embedding_size]\n",
    "        # [batch_size, windows * word_embedding_size]\n",
    "        input_c_context_vec_flat = tf.reshape(input_c_context_vec, [-1, windows * word_embedding_size])\n",
    "        # print input_c_context_vec_flat\n",
    "        # [batch_size, windows * word_embedding_size]\n",
    "        input_t_context_vec_flat = tf.reshape(input_t_context_vec, [-1, windows * word_embedding_size])\n",
    "        # print input_t_context_vec_flat\n",
    "        # print h_pool_flat\n",
    "        # 原本的论文将lexical leval features和sentence leval features组合\n",
    "        # 这里采用multi-task的思路 sentence leval features主要应用于事件的发现\n",
    "        # lexical leval features主要应用于角色的分类\n",
    "        input_sentence_features = h_pool_flat\n",
    "        input_lexical_features = tf.concat(1, [input_c_context_vec_flat, input_t_context_vec_flat])\n",
    "        # 总体的分类器 经过一层dropout 然后再送入softmax\n",
    "        with tf.device('/gpu:0'), tf.name_scope('dropout'):\n",
    "            input_sentence_features_dropout = tf.nn.dropout(input_sentence_features, dropout_keep_prob)   \n",
    "            input_lexical_features_dropout = tf.nn.dropout(input_lexical_features, dropout_keep_prob)\n",
    "        # 分类器\n",
    "        with tf.device('/gpu:0'), tf.name_scope('softmax'):\n",
    "            # num_filters_total是卷积之后的结果 sentence level features\n",
    "            # 事件类别分类\n",
    "            W1 = tf.Variable(tf.truncated_normal([num_filters_total, event_num_labels], stddev=0.1), name=\"W1\")\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[event_num_labels]), name=\"b1\")\n",
    "            xw1 = tf.nn.xw_plus_b(input_sentence_features, W1, b1)\n",
    "            scores_event = tf.nn.softmax(xw1, name=\"scores_event\")\n",
    "            # 2*windows*word_embedding_size是lexical leval features 具体的请看论文当中的详细介绍\n",
    "            predicts_event = tf.arg_max(scores_event, dimension=1, name=\"predicts_event\")\n",
    "            self.scores_event = scores_event\n",
    "            self.predicts_event = predicts_event\n",
    "            # 角色类别分类\n",
    "            W2 = tf.Variable(tf.truncated_normal([num_filters_total+2*windows*word_embedding_size+event_num_labels, role_num_labels], stddev=0.1), name=\"W2\")\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[role_num_labels]), name=\"b2\")\n",
    "            all_input_fatures = tf.concat(1, [input_c_context_vec_flat, input_t_context_vec_flat, h_pool_flat, scores_event])\n",
    "            xw2 = tf.nn.xw_plus_b(all_input_fatures, W2, b2)\n",
    "            scores_role = tf.nn.softmax(xw2, name=\"scores_role\")\n",
    "            predicts_role = tf.arg_max(scores_role, dimension=1, name=\"predicts_role\")\n",
    "            self.scores_role = scores_role\n",
    "            self.predicts_role = predicts_role\n",
    "        # 模型的代价函数 交叉熵代价函数\n",
    "        with tf.device('/gpu:0'), tf.name_scope('loss'):\n",
    "            # 事件的交叉熵代价函数\n",
    "            entropy_event = tf.nn.softmax_cross_entropy_with_logits(scores_event, input_event_y)\n",
    "            loss_event = tf.reduce_mean(entropy_event)\n",
    "            self.loss_event = loss_event\n",
    "            # 角色的交叉熵代价函数\n",
    "            entropy_role = tf.nn.softmax_cross_entropy_with_logits(scores_role, input_role_y)\n",
    "            loss_role = tf.reduce_mean(entropy_role)\n",
    "            self.loss_role = loss_role\n",
    "        # 准确度 用于每一次训练时调用\n",
    "        with tf.device('/gpu:0'), tf.name_scope(\"accuracy\"):\n",
    "            # 事件的准确度\n",
    "            correct_event = tf.equal(predicts_event, tf.argmax(input_event_y, 1))\n",
    "            accuracy_event = tf.reduce_mean(tf.cast(correct_event, \"float\"), name=\"accuracy_event\")\n",
    "            self.accuracy_event = accuracy_event\n",
    "            # 角色的准确度\n",
    "            correct_role = tf.equal(predicts_role, tf.argmax(input_role_y, 1))\n",
    "            accuracy_role = tf.reduce_mean(tf.cast(correct_role, \"float\"), name=\"accuracy_role\")\n",
    "            self.accuracy_role = accuracy_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/zhangluoyang/PycharmProjects/tensorflow/ace/ace_cnn_model_02/1481821052\n",
      "\n",
      "0: , loss 2.22794, acc 0.1\n",
      "0: , loss 2.24756, acc 0.05\n",
      "0: , loss 2.23091, acc 0.05\n",
      "0: , loss 2.09718, acc 0.25\n",
      "0: , loss 2.02209, acc 0.3\n",
      "0: , loss 2.00218, acc 0.35\n",
      "0: , loss 2.21765, acc 0.15\n",
      "0: , loss 2.21215, acc 0.1\n",
      "0: , loss 2.17783, acc 0.1\n",
      "0: , loss 2.12118, acc 0.25\n",
      "0: , loss 2.02357, acc 0.2\n",
      "0: , loss 1.84178, acc 0.55\n",
      "0: , loss 2.25538, acc 0.1\n",
      "0: , loss 1.87373, acc 0.5\n",
      "0: , loss 1.63846, acc 0.75\n",
      "0: , loss 1.73001, acc 0.65\n",
      "0: , loss 1.87905, acc 0.5\n",
      "0: , loss 1.96735, acc 0.4\n",
      "0: , loss 1.77172, acc 0.6\n",
      "0: , loss 1.87058, acc 0.5\n",
      "0: , loss 2.22169, acc 0.15\n",
      "0: , loss 2.12117, acc 0.25\n",
      "0: , loss 2.07027, acc 0.3\n",
      "0: , loss 2.22168, acc 0.15\n",
      "0: , loss 1.47209, acc 0.9\n",
      "0: , loss 1.82193, acc 0.55\n",
      "0: , loss 2.37176, acc 0\n",
      "0: , loss 2.37173, acc 0\n",
      "0: , loss 2.17151, acc 0.2\n",
      "0: , loss 2.37164, acc 0\n",
      "0: , loss 2.37143, acc 0\n",
      "0: , loss 2.37158, acc 0\n",
      "0: , loss 2.37176, acc 0\n",
      "0: , loss 2.37182, acc 0\n",
      "0: , loss 2.37182, acc 0\n",
      "0: , loss 1.47202, acc 0.9\n",
      "0: , loss 1.57223, acc 0.8\n",
      "0: , loss 1.87138, acc 0.5\n",
      "0: , loss 2.02189, acc 0.35\n",
      "0: , loss 2.02178, acc 0.35\n",
      "0: , loss 2.022, acc 0.35\n",
      "0: , loss 2.07192, acc 0.3\n",
      "0: , loss 1.57196, acc 0.8\n",
      "1: , loss 1.97025, acc 0.4\n",
      "1: , loss 2.0712, acc 0.3\n",
      "1: , loss 2.1719, acc 0.2\n",
      "1: , loss 2.04273, acc 0.35\n",
      "1: , loss 1.67198, acc 0.7\n",
      "1: , loss 2.16984, acc 0.2\n",
      "1: , loss 1.96949, acc 0.4\n",
      "1: , loss 2.17179, acc 0.2\n",
      "1: , loss 2.22021, acc 0.15\n",
      "1: , loss 2.06588, acc 0.3\n",
      "1: , loss 1.96636, acc 0.4\n",
      "1: , loss 2.02079, acc 0.35\n",
      "1: , loss 1.97189, acc 0.4\n",
      "1: , loss 1.92069, acc 0.45\n",
      "1: , loss 1.92161, acc 0.45\n",
      "1: , loss 1.87081, acc 0.5\n",
      "1: , loss 2.05219, acc 0.3\n",
      "1: , loss 2.14136, acc 0.2\n",
      "1: , loss 1.93773, acc 0.45\n",
      "1: , loss 2.0662, acc 0.3\n",
      "1: , loss 2.09288, acc 0.3\n",
      "1: , loss 1.96681, acc 0.4\n",
      "1: , loss 1.80037, acc 0.55\n",
      "1: , loss 1.9293, acc 0.45\n",
      "1: , loss 1.79531, acc 0.55\n",
      "1: , loss 1.96383, acc 0.4\n",
      "1: , loss 1.78969, acc 0.6\n",
      "1: , loss 2.04437, acc 0.35\n",
      "1: , loss 1.82135, acc 0.55\n",
      "1: , loss 2.03027, acc 0.35\n",
      "1: , loss 1.78478, acc 0.6\n",
      "1: , loss 2.11929, acc 0.2\n",
      "1: , loss 1.74118, acc 0.65\n",
      "1: , loss 2.07065, acc 0.3\n",
      "1: , loss 1.87813, acc 0.45\n",
      "1: , loss 2.01457, acc 0.35\n",
      "1: , loss 2.04102, acc 0.35\n",
      "1: , loss 1.81966, acc 0.55\n",
      "1: , loss 1.94884, acc 0.4\n",
      "1: , loss 2.00848, acc 0.35\n",
      "1: , loss 1.8168, acc 0.55\n",
      "1: , loss 1.76627, acc 0.6\n",
      "1: , loss 2.00989, acc 0.35\n",
      "2: , loss 1.80073, acc 0.55\n",
      "2: , loss 1.9552, acc 0.4\n",
      "2: , loss 1.86759, acc 0.5\n",
      "2: , loss 1.93453, acc 0.45\n",
      "2: , loss 1.80774, acc 0.6\n",
      "2: , loss 1.83922, acc 0.55\n",
      "2: , loss 1.75014, acc 0.6\n",
      "2: , loss 1.78427, acc 0.6\n",
      "2: , loss 1.69517, acc 0.7\n",
      "2: , loss 1.99492, acc 0.35\n",
      "2: , loss 1.81308, acc 0.55\n",
      "2: , loss 1.8325, acc 0.55\n",
      "2: , loss 1.76973, acc 0.6\n",
      "2: , loss 1.597, acc 0.75\n",
      "2: , loss 1.85623, acc 0.5\n",
      "2: , loss 1.71622, acc 0.65\n",
      "2: , loss 1.86637, acc 0.5\n",
      "2: , loss 1.8489, acc 0.55\n",
      "2: , loss 2.05573, acc 0.3\n",
      "2: , loss 1.91047, acc 0.45\n",
      "2: , loss 1.97843, acc 0.4\n",
      "2: , loss 1.8739, acc 0.5\n",
      "2: , loss 1.95868, acc 0.4\n",
      "2: , loss 1.92109, acc 0.45\n",
      "2: , loss 1.81806, acc 0.55\n",
      "2: , loss 1.86402, acc 0.5\n",
      "2: , loss 1.95971, acc 0.4\n",
      "2: , loss 1.71973, acc 0.65\n",
      "2: , loss 1.76651, acc 0.6\n",
      "2: , loss 1.80932, acc 0.55\n",
      "2: , loss 2.01868, acc 0.35\n",
      "2: , loss 1.90927, acc 0.45\n",
      "2: , loss 1.81778, acc 0.55\n",
      "2: , loss 1.86134, acc 0.5\n",
      "2: , loss 1.85801, acc 0.5\n",
      "2: , loss 1.81531, acc 0.55\n",
      "2: , loss 1.96158, acc 0.4\n",
      "2: , loss 1.95899, acc 0.4\n",
      "2: , loss 1.80926, acc 0.55\n",
      "2: , loss 1.72669, acc 0.65\n",
      "2: , loss 2.00328, acc 0.35\n",
      "2: , loss 1.8624, acc 0.5\n",
      "2: , loss 2.00339, acc 0.35\n",
      "3: , loss 1.90945, acc 0.45\n",
      "3: , loss 1.75847, acc 0.6\n",
      "3: , loss 1.81113, acc 0.55\n",
      "3: , loss 1.80688, acc 0.55\n",
      "3: , loss 1.90414, acc 0.45\n",
      "3: , loss 1.90739, acc 0.45\n",
      "3: , loss 1.7141, acc 0.65\n",
      "3: , loss 1.86042, acc 0.5\n",
      "3: , loss 1.7587, acc 0.6\n",
      "3: , loss 1.80836, acc 0.55\n",
      "3: , loss 1.90042, acc 0.45\n",
      "3: , loss 1.81012, acc 0.55\n",
      "3: , loss 1.71258, acc 0.65\n",
      "3: , loss 1.61162, acc 0.75\n",
      "3: , loss 1.71126, acc 0.65\n",
      "3: , loss 2.00168, acc 0.35\n",
      "3: , loss 1.90298, acc 0.45\n",
      "3: , loss 1.90105, acc 0.45\n",
      "3: , loss 1.80165, acc 0.55\n",
      "3: , loss 1.85518, acc 0.5\n",
      "3: , loss 1.76363, acc 0.6\n",
      "3: , loss 1.9079, acc 0.45\n",
      "3: , loss 1.99181, acc 0.35\n",
      "3: , loss 1.8348, acc 0.55\n",
      "3: , loss 1.60932, acc 0.8\n",
      "3: , loss 1.97046, acc 0.4\n",
      "3: , loss 1.74045, acc 0.65\n",
      "3: , loss 1.73886, acc 0.65\n",
      "3: , loss 1.98788, acc 0.35\n",
      "3: , loss 1.79094, acc 0.55\n",
      "3: , loss 1.83846, acc 0.5\n",
      "3: , loss 2.02215, acc 0.35\n",
      "3: , loss 1.74978, acc 0.65\n",
      "3: , loss 1.6636, acc 0.8\n",
      "3: , loss 1.7738, acc 0.6\n",
      "3: , loss 1.69751, acc 0.75\n",
      "3: , loss 1.78328, acc 0.6\n",
      "3: , loss 1.5977, acc 0.85\n",
      "3: , loss 1.62636, acc 0.75\n",
      "3: , loss 1.76966, acc 0.6\n",
      "3: , loss 1.72288, acc 0.75\n",
      "3: , loss 1.66357, acc 0.75\n",
      "3: , loss 1.76705, acc 0.6\n",
      "4: , loss 1.87838, acc 0.5\n",
      "4: , loss 1.62738, acc 0.75\n",
      "4: , loss 1.52937, acc 0.85\n",
      "4: , loss 1.58405, acc 0.8\n",
      "4: , loss 1.77765, acc 0.55\n",
      "4: , loss 1.57023, acc 0.8\n",
      "4: , loss 1.5775, acc 0.8\n",
      "4: , loss 1.65662, acc 0.7\n",
      "4: , loss 1.72878, acc 0.6\n",
      "4: , loss 1.80661, acc 0.6\n",
      "4: , loss 1.60562, acc 0.8\n",
      "4: , loss 1.68501, acc 0.75\n",
      "4: , loss 1.88179, acc 0.5\n",
      "4: , loss 1.58649, acc 0.85\n",
      "4: , loss 1.63735, acc 0.75\n",
      "4: , loss 1.74279, acc 0.65\n",
      "4: , loss 1.62109, acc 0.8\n",
      "4: , loss 1.65682, acc 0.7\n",
      "4: , loss 1.62204, acc 0.75\n",
      "4: , loss 1.56964, acc 0.8\n",
      "4: , loss 1.64076, acc 0.75\n",
      "4: , loss 1.59662, acc 0.75\n",
      "4: , loss 1.63259, acc 0.75\n",
      "4: , loss 1.53045, acc 0.85\n",
      "4: , loss 1.48801, acc 0.9\n",
      "4: , loss 1.63094, acc 0.75\n",
      "4: , loss 1.65273, acc 0.75\n",
      "4: , loss 1.60389, acc 0.75\n",
      "4: , loss 1.52362, acc 0.85\n",
      "4: , loss 1.71673, acc 0.65\n",
      "4: , loss 1.65057, acc 0.7\n",
      "4: , loss 1.53789, acc 0.85\n",
      "4: , loss 1.70187, acc 0.65\n",
      "4: , loss 1.56208, acc 0.85\n",
      "4: , loss 1.631, acc 0.75\n",
      "4: , loss 1.5466, acc 0.85\n",
      "4: , loss 1.63526, acc 0.85\n",
      "4: , loss 1.57901, acc 0.85\n",
      "4: , loss 1.54104, acc 0.85\n",
      "4: , loss 1.47695, acc 0.95\n",
      "4: , loss 1.50783, acc 0.9\n",
      "4: , loss 1.72618, acc 0.65\n",
      "4: , loss 1.67852, acc 0.7\n",
      "5: , loss 1.50119, acc 0.9\n",
      "5: , loss 1.57991, acc 0.8\n",
      "5: , loss 1.65346, acc 0.7\n",
      "5: , loss 1.46726, acc 0.9\n",
      "5: , loss 1.48513, acc 0.9\n",
      "5: , loss 1.42659, acc 0.95\n",
      "5: , loss 1.56549, acc 0.8\n",
      "5: , loss 1.62768, acc 0.7\n",
      "5: , loss 1.42536, acc 0.95\n",
      "5: , loss 1.52964, acc 0.85\n",
      "5: , loss 1.58927, acc 0.8\n",
      "5: , loss 1.4853, acc 0.9\n",
      "5: , loss 1.56693, acc 0.8\n",
      "5: , loss 1.56891, acc 0.8\n",
      "5: , loss 1.47502, acc 0.9\n",
      "5: , loss 1.47205, acc 0.9\n",
      "5: , loss 1.58959, acc 0.8\n",
      "5: , loss 1.567, acc 0.8\n",
      "5: , loss 1.61834, acc 0.8\n",
      "5: , loss 1.48752, acc 0.9\n",
      "5: , loss 1.62498, acc 0.75\n",
      "5: , loss 1.51592, acc 0.85\n",
      "5: , loss 1.5182, acc 0.85\n",
      "5: , loss 1.52704, acc 0.85\n",
      "5: , loss 1.57152, acc 0.8\n",
      "5: , loss 1.56687, acc 0.8\n",
      "5: , loss 1.46274, acc 0.9\n",
      "5: , loss 1.5201, acc 0.85\n",
      "5: , loss 1.57772, acc 0.8\n",
      "5: , loss 1.5361, acc 0.85\n",
      "5: , loss 1.51938, acc 0.85\n",
      "5: , loss 1.55472, acc 0.8\n",
      "5: , loss 1.47167, acc 0.9\n",
      "5: , loss 1.48911, acc 0.9\n",
      "5: , loss 1.43978, acc 0.95\n",
      "5: , loss 1.43391, acc 0.95\n",
      "5: , loss 1.61251, acc 0.75\n",
      "5: , loss 1.57548, acc 0.8\n",
      "5: , loss 1.56483, acc 0.8\n",
      "5: , loss 1.51303, acc 0.85\n",
      "5: , loss 1.58087, acc 0.8\n",
      "5: , loss 1.51568, acc 0.85\n",
      "5: , loss 1.52869, acc 0.85\n",
      "6: , loss 1.42362, acc 0.95\n",
      "6: , loss 1.51885, acc 0.85\n",
      "6: , loss 1.51844, acc 0.85\n",
      "6: , loss 1.59487, acc 0.75\n",
      "6: , loss 1.42196, acc 0.95\n",
      "6: , loss 1.52559, acc 0.85\n",
      "6: , loss 1.54524, acc 0.85\n",
      "6: , loss 1.56397, acc 0.8\n",
      "6: , loss 1.51999, acc 0.85\n",
      "6: , loss 1.4711, acc 0.9\n",
      "6: , loss 1.42613, acc 0.95\n",
      "6: , loss 1.51805, acc 0.85\n",
      "6: , loss 1.42187, acc 0.95\n",
      "6: , loss 1.37698, acc 1\n",
      "6: , loss 1.52861, acc 0.85\n",
      "6: , loss 1.42323, acc 0.95\n",
      "6: , loss 1.6111, acc 0.75\n",
      "6: , loss 1.51365, acc 0.85\n",
      "6: , loss 1.46904, acc 0.9\n",
      "6: , loss 1.47138, acc 0.9\n",
      "6: , loss 1.51822, acc 0.85\n",
      "6: , loss 1.63524, acc 0.75\n",
      "6: , loss 1.42373, acc 0.95\n",
      "6: , loss 1.51725, acc 0.85\n",
      "6: , loss 1.42344, acc 0.95\n",
      "6: , loss 1.58519, acc 0.8\n",
      "6: , loss 1.515, acc 0.85\n",
      "6: , loss 1.57904, acc 0.8\n",
      "6: , loss 1.56379, acc 0.8\n",
      "6: , loss 1.46998, acc 0.9\n",
      "6: , loss 1.37504, acc 1\n",
      "6: , loss 1.47144, acc 0.9\n",
      "6: , loss 1.56521, acc 0.8\n",
      "6: , loss 1.56371, acc 0.8\n",
      "6: , loss 1.42648, acc 0.95\n",
      "6: , loss 1.56292, acc 0.8\n",
      "6: , loss 1.60751, acc 0.75\n",
      "6: , loss 1.5478, acc 0.85\n",
      "6: , loss 1.56347, acc 0.8\n",
      "6: , loss 1.51899, acc 0.85\n",
      "6: , loss 1.3738, acc 1\n",
      "6: , loss 1.61451, acc 0.75\n",
      "6: , loss 1.57314, acc 0.8\n",
      "7: , loss 1.56281, acc 0.8\n",
      "7: , loss 1.42121, acc 0.95\n",
      "7: , loss 1.46822, acc 0.9\n",
      "7: , loss 1.56304, acc 0.8\n",
      "7: , loss 1.42186, acc 0.95\n",
      "7: , loss 1.65667, acc 0.7\n",
      "7: , loss 1.51548, acc 0.85\n",
      "7: , loss 1.51554, acc 0.85\n",
      "7: , loss 1.5178, acc 0.85\n",
      "7: , loss 1.51523, acc 0.85\n",
      "7: , loss 1.37424, acc 1\n",
      "7: , loss 1.46805, acc 0.9\n",
      "7: , loss 1.51508, acc 0.85\n",
      "7: , loss 1.42009, acc 0.95\n",
      "7: , loss 1.61176, acc 0.75\n",
      "7: , loss 1.37464, acc 1\n",
      "7: , loss 1.51502, acc 0.85\n",
      "7: , loss 1.51608, acc 0.85\n",
      "7: , loss 1.51499, acc 0.85\n",
      "7: , loss 1.51635, acc 0.85\n",
      "7: , loss 1.51482, acc 0.85\n",
      "7: , loss 1.4218, acc 0.95\n",
      "7: , loss 1.56024, acc 0.8\n",
      "7: , loss 1.42019, acc 0.95\n",
      "7: , loss 1.46759, acc 0.9\n",
      "7: , loss 1.58117, acc 0.8\n",
      "7: , loss 1.51528, acc 0.85\n",
      "7: , loss 1.46896, acc 0.9\n",
      "7: , loss 1.46856, acc 0.9\n",
      "7: , loss 1.56698, acc 0.8\n",
      "7: , loss 1.61071, acc 0.75\n",
      "7: , loss 1.42026, acc 0.95\n",
      "7: , loss 1.49474, acc 0.9\n",
      "7: , loss 1.46751, acc 0.9\n",
      "7: , loss 1.60878, acc 0.75\n",
      "7: , loss 1.6093, acc 0.75\n",
      "7: , loss 1.4681, acc 0.9\n",
      "7: , loss 1.3762, acc 1\n",
      "7: , loss 1.54525, acc 0.85\n",
      "7: , loss 1.53873, acc 0.85\n",
      "7: , loss 1.51932, acc 0.85\n",
      "7: , loss 1.51717, acc 0.85\n",
      "7: , loss 1.42591, acc 0.95\n",
      "8: , loss 1.44106, acc 0.95\n",
      "8: , loss 1.5649, acc 0.8\n",
      "8: , loss 1.66299, acc 0.7\n",
      "8: , loss 1.51881, acc 0.85\n",
      "8: , loss 1.47374, acc 0.9\n",
      "8: , loss 1.47724, acc 0.9\n",
      "8: , loss 1.47247, acc 0.9\n",
      "8: , loss 1.42478, acc 0.95\n",
      "8: , loss 1.4222, acc 0.95\n",
      "8: , loss 1.56648, acc 0.8\n",
      "8: , loss 1.61039, acc 0.75\n",
      "8: , loss 1.46795, acc 0.9\n",
      "8: , loss 1.51521, acc 0.85\n",
      "8: , loss 1.51537, acc 0.85\n",
      "8: , loss 1.46829, acc 0.9\n",
      "8: , loss 1.51903, acc 0.85\n",
      "8: , loss 1.56158, acc 0.8\n",
      "8: , loss 1.56324, acc 0.8\n",
      "8: , loss 1.5623, acc 0.8\n",
      "8: , loss 1.5151, acc 0.85\n",
      "8: , loss 1.42024, acc 0.95\n",
      "8: , loss 1.56205, acc 0.8\n",
      "8: , loss 1.42081, acc 0.95\n",
      "8: , loss 1.46785, acc 0.9\n",
      "8: , loss 1.4211, acc 0.95\n",
      "8: , loss 1.56305, acc 0.8\n",
      "8: , loss 1.51546, acc 0.85\n",
      "8: , loss 1.41997, acc 0.95\n",
      "8: , loss 1.56261, acc 0.8\n",
      "8: , loss 1.65642, acc 0.7\n",
      "8: , loss 1.46798, acc 0.9\n",
      "8: , loss 1.37315, acc 1\n",
      "8: , loss 1.56233, acc 0.8\n",
      "8: , loss 1.42058, acc 0.95\n",
      "8: , loss 1.46758, acc 0.9\n",
      "8: , loss 1.3745, acc 1\n",
      "8: , loss 1.612, acc 0.75\n",
      "8: , loss 1.51541, acc 0.85\n",
      "8: , loss 1.6565, acc 0.7\n",
      "8: , loss 1.47065, acc 0.9\n",
      "8: , loss 1.42192, acc 0.95\n",
      "8: , loss 1.3728, acc 1\n",
      "8: , loss 1.42055, acc 0.95\n",
      "9: , loss 1.51453, acc 0.85\n",
      "9: , loss 1.51437, acc 0.85\n",
      "9: , loss 1.46835, acc 0.9\n",
      "9: , loss 1.4683, acc 0.9\n",
      "9: , loss 1.51416, acc 0.85\n",
      "9: , loss 1.56209, acc 0.8\n",
      "9: , loss 1.51508, acc 0.85\n",
      "9: , loss 1.51423, acc 0.85\n",
      "9: , loss 1.46735, acc 0.9\n",
      "9: , loss 1.46762, acc 0.9\n",
      "9: , loss 1.46742, acc 0.9\n",
      "9: , loss 1.56109, acc 0.8\n",
      "9: , loss 1.37331, acc 1\n",
      "9: , loss 1.51709, acc 0.85\n",
      "9: , loss 1.37373, acc 1\n",
      "9: , loss 1.56188, acc 0.8\n",
      "9: , loss 1.51564, acc 0.85\n",
      "9: , loss 1.51654, acc 0.85\n",
      "9: , loss 1.46719, acc 0.9\n",
      "9: , loss 1.42041, acc 0.95\n",
      "9: , loss 1.46705, acc 0.9\n",
      "9: , loss 1.51458, acc 0.85\n",
      "9: , loss 1.5147, acc 0.85\n",
      "9: , loss 1.42033, acc 0.95\n",
      "9: , loss 1.60802, acc 0.75\n",
      "9: , loss 1.60848, acc 0.75\n",
      "9: , loss 1.56367, acc 0.8\n",
      "9: , loss 1.46679, acc 0.9\n",
      "9: , loss 1.46763, acc 0.9\n",
      "9: , loss 1.41991, acc 0.95\n",
      "9: , loss 1.51408, acc 0.85\n",
      "9: , loss 1.56036, acc 0.8\n",
      "9: , loss 1.51528, acc 0.85\n",
      "9: , loss 1.51762, acc 0.85\n",
      "9: , loss 1.51454, acc 0.85\n",
      "9: , loss 1.60924, acc 0.75\n",
      "9: , loss 1.51451, acc 0.85\n",
      "9: , loss 1.42002, acc 0.95\n",
      "9: , loss 1.51472, acc 0.85\n",
      "9: , loss 1.51437, acc 0.85\n",
      "9: , loss 1.46733, acc 0.9\n",
      "9: , loss 1.46737, acc 0.9\n",
      "9: , loss 1.42015, acc 0.95\n",
      "10: , loss 1.51426, acc 0.85\n",
      "10: , loss 1.42033, acc 0.95\n",
      "10: , loss 1.51418, acc 0.85\n",
      "10: , loss 1.46725, acc 0.9\n",
      "10: , loss 1.61382, acc 0.75\n",
      "10: , loss 1.6079, acc 0.75\n",
      "10: , loss 1.42007, acc 0.95\n",
      "10: , loss 1.51393, acc 0.85\n",
      "10: , loss 1.56317, acc 0.8\n",
      "10: , loss 1.46802, acc 0.9\n",
      "10: , loss 1.60782, acc 0.75\n",
      "10: , loss 1.51445, acc 0.85\n",
      "10: , loss 1.46696, acc 0.9\n",
      "10: , loss 1.37268, acc 1\n",
      "10: , loss 1.46766, acc 0.9\n",
      "10: , loss 1.56136, acc 0.8\n",
      "10: , loss 1.51429, acc 0.85\n",
      "10: , loss 1.51526, acc 0.85\n",
      "10: , loss 1.46718, acc 0.9\n",
      "10: , loss 1.56117, acc 0.8\n",
      "10: , loss 1.46692, acc 0.9\n",
      "10: , loss 1.51392, acc 0.85\n",
      "10: , loss 1.46725, acc 0.9\n",
      "10: , loss 1.41971, acc 0.95\n",
      "10: , loss 1.41989, acc 0.95\n",
      "10: , loss 1.42024, acc 0.95\n",
      "10: , loss 1.56107, acc 0.8\n",
      "10: , loss 1.65495, acc 0.7\n",
      "10: , loss 1.42047, acc 0.95\n",
      "10: , loss 1.41995, acc 0.95\n",
      "10: , loss 1.56196, acc 0.8\n",
      "10: , loss 1.5146, acc 0.85\n",
      "10: , loss 1.37307, acc 1\n",
      "10: , loss 1.4205, acc 0.95\n",
      "10: , loss 1.46664, acc 0.9\n",
      "10: , loss 1.51723, acc 0.85\n",
      "10: , loss 1.51425, acc 0.85\n",
      "10: , loss 1.46717, acc 0.9\n",
      "10: , loss 1.56084, acc 0.8\n",
      "10: , loss 1.51459, acc 0.85\n",
      "10: , loss 1.46718, acc 0.9\n",
      "10: , loss 1.46353, acc 0.9\n",
      "10: , loss 1.60805, acc 0.75\n",
      "11: , loss 1.46977, acc 0.9\n",
      "11: , loss 1.37286, acc 1\n",
      "11: , loss 1.46718, acc 0.9\n",
      "11: , loss 1.51375, acc 0.85\n",
      "11: , loss 1.46718, acc 0.9\n",
      "11: , loss 1.37275, acc 1\n",
      "11: , loss 1.56106, acc 0.8\n",
      "11: , loss 1.4669, acc 0.9\n",
      "11: , loss 1.51429, acc 0.85\n",
      "11: , loss 1.60985, acc 0.75\n",
      "11: , loss 1.51381, acc 0.85\n",
      "11: , loss 1.37299, acc 1\n",
      "11: , loss 1.41995, acc 0.95\n",
      "11: , loss 1.5613, acc 0.8\n",
      "11: , loss 1.51435, acc 0.85\n",
      "11: , loss 1.46649, acc 0.9\n",
      "11: , loss 1.51358, acc 0.85\n",
      "11: , loss 1.70176, acc 0.65\n",
      "11: , loss 1.60881, acc 0.75\n",
      "11: , loss 1.48832, acc 0.9\n",
      "11: , loss 1.46696, acc 0.9\n",
      "11: , loss 1.41986, acc 0.95\n",
      "11: , loss 1.51425, acc 0.85\n",
      "11: , loss 1.42015, acc 0.95\n",
      "11: , loss 1.56171, acc 0.8\n",
      "11: , loss 1.46851, acc 0.9\n",
      "11: , loss 1.46716, acc 0.9\n",
      "11: , loss 1.46795, acc 0.9\n",
      "11: , loss 1.42029, acc 0.95\n",
      "11: , loss 1.56133, acc 0.8\n",
      "11: , loss 1.51614, acc 0.85\n",
      "11: , loss 1.56364, acc 0.8\n",
      "11: , loss 1.56175, acc 0.8\n",
      "11: , loss 1.51475, acc 0.85\n",
      "11: , loss 1.51403, acc 0.85\n",
      "11: , loss 1.46713, acc 0.9\n",
      "11: , loss 1.41977, acc 0.95\n",
      "11: , loss 1.51417, acc 0.85\n",
      "11: , loss 1.46992, acc 0.9\n",
      "11: , loss 1.46784, acc 0.9\n",
      "11: , loss 1.41998, acc 0.95\n",
      "11: , loss 1.51371, acc 0.85\n",
      "11: , loss 1.60819, acc 0.75\n",
      "12: , loss 1.51426, acc 0.85\n",
      "12: , loss 1.46648, acc 0.9\n",
      "12: , loss 1.51467, acc 0.85\n",
      "12: , loss 1.46702, acc 0.9\n",
      "12: , loss 1.4668, acc 0.9\n",
      "12: , loss 1.51423, acc 0.85\n",
      "12: , loss 1.7054, acc 0.65\n",
      "12: , loss 1.46756, acc 0.9\n",
      "12: , loss 1.51421, acc 0.85\n",
      "12: , loss 1.51445, acc 0.85\n",
      "12: , loss 1.46693, acc 0.9\n",
      "12: , loss 1.56095, acc 0.8\n",
      "12: , loss 1.46679, acc 0.9\n",
      "12: , loss 1.60819, acc 0.75\n",
      "12: , loss 1.467, acc 0.9\n",
      "12: , loss 1.37237, acc 1\n",
      "12: , loss 1.51369, acc 0.85\n",
      "12: , loss 1.37265, acc 1\n",
      "12: , loss 1.51351, acc 0.85\n",
      "12: , loss 1.41949, acc 0.95\n",
      "12: , loss 1.42082, acc 0.95\n",
      "12: , loss 1.46966, acc 0.9\n",
      "12: , loss 1.70615, acc 0.65\n",
      "12: , loss 1.46675, acc 0.9\n",
      "12: , loss 1.60772, acc 0.75\n",
      "12: , loss 1.41951, acc 0.95\n",
      "12: , loss 1.7025, acc 0.65\n",
      "12: , loss 1.56067, acc 0.8\n",
      "12: , loss 1.51395, acc 0.85\n",
      "12: , loss 1.56109, acc 0.8\n",
      "12: , loss 1.41997, acc 0.95\n",
      "12: , loss 1.37267, acc 1\n",
      "12: , loss 1.46661, acc 0.9\n",
      "12: , loss 1.51355, acc 0.85\n",
      "12: , loss 1.51406, acc 0.85\n",
      "12: , loss 1.46698, acc 0.9\n",
      "12: , loss 1.46659, acc 0.9\n",
      "12: , loss 1.4196, acc 0.95\n",
      "12: , loss 1.46741, acc 0.9\n",
      "12: , loss 1.51463, acc 0.85\n",
      "12: , loss 1.46684, acc 0.9\n",
      "12: , loss 1.41982, acc 0.95\n",
      "12: , loss 1.46687, acc 0.9\n",
      "13: , loss 1.51435, acc 0.85\n",
      "13: , loss 1.4667, acc 0.9\n",
      "13: , loss 1.41983, acc 0.95\n",
      "13: , loss 1.51647, acc 0.85\n",
      "13: , loss 1.51358, acc 0.85\n",
      "13: , loss 1.46643, acc 0.9\n",
      "13: , loss 1.46706, acc 0.9\n",
      "13: , loss 1.56085, acc 0.8\n",
      "13: , loss 1.46659, acc 0.9\n",
      "13: , loss 1.42325, acc 0.95\n",
      "13: , loss 1.65486, acc 0.7\n",
      "13: , loss 1.46652, acc 0.9\n",
      "13: , loss 1.51382, acc 0.85\n",
      "13: , loss 1.41992, acc 0.95\n",
      "13: , loss 1.51406, acc 0.85\n",
      "13: , loss 1.46693, acc 0.9\n",
      "13: , loss 1.56085, acc 0.8\n",
      "13: , loss 1.56059, acc 0.8\n",
      "13: , loss 1.41961, acc 0.95\n",
      "13: , loss 1.41953, acc 0.95\n",
      "13: , loss 1.51388, acc 0.85\n",
      "13: , loss 1.46711, acc 0.9\n",
      "13: , loss 1.46684, acc 0.9\n",
      "13: , loss 1.60797, acc 0.75\n",
      "13: , loss 1.46688, acc 0.9\n",
      "13: , loss 1.51383, acc 0.85\n",
      "13: , loss 1.60807, acc 0.75\n",
      "13: , loss 1.60738, acc 0.75\n",
      "13: , loss 1.51383, acc 0.85\n",
      "13: , loss 1.46696, acc 0.9\n",
      "13: , loss 1.46645, acc 0.9\n",
      "13: , loss 1.41985, acc 0.95\n",
      "13: , loss 1.60779, acc 0.75\n",
      "13: , loss 1.46673, acc 0.9\n",
      "13: , loss 1.46649, acc 0.9\n",
      "13: , loss 1.5133, acc 0.85\n",
      "13: , loss 1.51351, acc 0.85\n",
      "13: , loss 1.51352, acc 0.85\n",
      "13: , loss 1.46677, acc 0.9\n",
      "13: , loss 1.5136, acc 0.85\n",
      "13: , loss 1.41997, acc 0.95\n",
      "13: , loss 1.46943, acc 0.9\n",
      "13: , loss 1.41976, acc 0.95\n",
      "14: , loss 1.6544, acc 0.7\n",
      "14: , loss 1.56057, acc 0.8\n",
      "14: , loss 1.70447, acc 0.65\n",
      "14: , loss 1.46676, acc 0.9\n",
      "14: , loss 1.42008, acc 0.95\n",
      "14: , loss 1.41967, acc 0.95\n",
      "14: , loss 1.51366, acc 0.85\n",
      "14: , loss 1.37271, acc 1\n",
      "14: , loss 1.56018, acc 0.8\n",
      "14: , loss 1.56048, acc 0.8\n",
      "14: , loss 1.46627, acc 0.9\n",
      "14: , loss 1.5136, acc 0.85\n",
      "14: , loss 1.46644, acc 0.9\n",
      "14: , loss 1.46676, acc 0.9\n",
      "14: , loss 1.4665, acc 0.9\n",
      "14: , loss 1.46654, acc 0.9\n",
      "14: , loss 1.51211, acc 0.85\n",
      "14: , loss 1.51351, acc 0.85\n",
      "14: , loss 1.41994, acc 0.95\n",
      "14: , loss 1.5132, acc 0.85\n",
      "14: , loss 1.51319, acc 0.85\n",
      "14: , loss 1.51228, acc 0.85\n",
      "14: , loss 1.41971, acc 0.95\n",
      "14: , loss 1.55896, acc 0.8\n",
      "14: , loss 1.46677, acc 0.9\n",
      "14: , loss 1.51378, acc 0.85\n",
      "14: , loss 1.46486, acc 0.9\n",
      "14: , loss 1.51356, acc 0.85\n",
      "14: , loss 1.60267, acc 0.75\n",
      "14: , loss 1.56062, acc 0.8\n",
      "14: , loss 1.46633, acc 0.9\n",
      "14: , loss 1.37317, acc 1\n",
      "14: , loss 1.56041, acc 0.8\n",
      "14: , loss 1.46709, acc 0.9\n",
      "14: , loss 1.47036, acc 0.9\n",
      "14: , loss 1.49203, acc 0.9\n",
      "14: , loss 1.4207, acc 0.95\n",
      "14: , loss 1.42105, acc 0.95\n",
      "14: , loss 1.37637, acc 1\n",
      "14: , loss 1.60643, acc 0.75\n",
      "14: , loss 1.51771, acc 0.85\n",
      "14: , loss 1.37649, acc 1\n",
      "14: , loss 1.55369, acc 0.85\n",
      "15: , loss 1.5151, acc 0.85\n",
      "15: , loss 1.46867, acc 0.9\n",
      "15: , loss 1.46727, acc 0.9\n",
      "15: , loss 1.39108, acc 1\n",
      "15: , loss 1.42487, acc 0.95\n",
      "15: , loss 1.3799, acc 1\n",
      "15: , loss 1.42751, acc 0.95\n",
      "15: , loss 1.43107, acc 0.95\n",
      "15: , loss 1.51863, acc 0.85\n",
      "15: , loss 1.47336, acc 0.9\n",
      "15: , loss 1.47445, acc 0.9\n",
      "15: , loss 1.57447, acc 0.8\n",
      "15: , loss 1.52222, acc 0.85\n",
      "15: , loss 1.54903, acc 0.85\n",
      "15: , loss 1.47006, acc 0.9\n",
      "15: , loss 1.37771, acc 1\n",
      "15: , loss 1.47022, acc 0.9\n",
      "15: , loss 1.47095, acc 0.9\n",
      "15: , loss 1.42002, acc 0.95\n",
      "15: , loss 1.37726, acc 1\n",
      "15: , loss 1.60311, acc 0.75\n",
      "15: , loss 1.46134, acc 0.9\n",
      "15: , loss 1.55881, acc 0.8\n",
      "15: , loss 1.37991, acc 1\n",
      "15: , loss 1.49842, acc 0.9\n",
      "15: , loss 1.44454, acc 0.95\n",
      "15: , loss 1.37778, acc 1\n",
      "15: , loss 1.52925, acc 0.9\n",
      "15: , loss 1.51182, acc 0.85\n",
      "15: , loss 1.44811, acc 0.95\n",
      "15: , loss 1.48582, acc 0.9\n",
      "15: , loss 1.3928, acc 1\n",
      "15: , loss 1.39319, acc 1\n",
      "15: , loss 1.44369, acc 0.95\n",
      "15: , loss 1.3794, acc 1\n",
      "15: , loss 1.37819, acc 1\n",
      "15: , loss 1.42634, acc 0.95\n",
      "15: , loss 1.37913, acc 1\n",
      "15: , loss 1.511, acc 0.9\n",
      "15: , loss 1.42238, acc 0.95\n",
      "15: , loss 1.57995, acc 0.8\n",
      "15: , loss 1.42769, acc 0.95\n",
      "15: , loss 1.47086, acc 0.9\n",
      "16: , loss 1.47028, acc 0.9\n",
      "16: , loss 1.44991, acc 0.95\n",
      "16: , loss 1.37517, acc 1\n",
      "16: , loss 1.42146, acc 0.95\n",
      "16: , loss 1.42096, acc 0.95\n",
      "16: , loss 1.5143, acc 0.85\n",
      "16: , loss 1.37432, acc 1\n",
      "16: , loss 1.37343, acc 1\n",
      "16: , loss 1.42364, acc 0.95\n",
      "16: , loss 1.37775, acc 1\n",
      "16: , loss 1.42188, acc 0.95\n",
      "16: , loss 1.37785, acc 1\n",
      "16: , loss 1.42104, acc 0.95\n",
      "16: , loss 1.37868, acc 1\n",
      "16: , loss 1.5162, acc 0.85\n",
      "16: , loss 1.42493, acc 0.95\n",
      "16: , loss 1.37899, acc 1\n",
      "16: , loss 1.46905, acc 0.9\n",
      "16: , loss 1.37291, acc 1\n",
      "16: , loss 1.51508, acc 0.85\n",
      "16: , loss 1.37293, acc 1\n",
      "16: , loss 1.42045, acc 0.95\n",
      "16: , loss 1.37305, acc 1\n",
      "16: , loss 1.46653, acc 0.9\n",
      "16: , loss 1.37891, acc 1\n",
      "16: , loss 1.42004, acc 0.95\n",
      "16: , loss 1.46704, acc 0.9\n",
      "16: , loss 1.45345, acc 0.9\n",
      "16: , loss 1.46941, acc 0.9\n",
      "16: , loss 1.37326, acc 1\n",
      "16: , loss 1.37473, acc 1\n",
      "16: , loss 1.3745, acc 1\n",
      "16: , loss 1.3732, acc 1\n",
      "16: , loss 1.43525, acc 0.95\n",
      "16: , loss 1.37404, acc 1\n",
      "16: , loss 1.37462, acc 1\n",
      "16: , loss 1.46888, acc 0.9\n",
      "16: , loss 1.37585, acc 1\n",
      "16: , loss 1.37518, acc 1\n",
      "16: , loss 1.44867, acc 0.95\n",
      "16: , loss 1.42072, acc 0.95\n",
      "16: , loss 1.42485, acc 0.95\n",
      "16: , loss 1.56641, acc 0.8\n",
      "17: , loss 1.47041, acc 0.9\n",
      "17: , loss 1.37623, acc 1\n",
      "17: , loss 1.42232, acc 0.95\n",
      "17: , loss 1.41998, acc 0.95\n",
      "17: , loss 1.46811, acc 0.9\n",
      "17: , loss 1.37422, acc 1\n",
      "17: , loss 1.3731, acc 1\n",
      "17: , loss 1.51581, acc 0.85\n",
      "17: , loss 1.46754, acc 0.9\n",
      "17: , loss 1.3836, acc 1\n",
      "17: , loss 1.37331, acc 1\n",
      "17: , loss 1.46686, acc 0.9\n",
      "17: , loss 1.41943, acc 0.95\n",
      "17: , loss 1.41998, acc 0.95\n",
      "17: , loss 1.41972, acc 0.95\n",
      "17: , loss 1.37233, acc 1\n",
      "17: , loss 1.41973, acc 0.95\n",
      "17: , loss 1.37243, acc 1\n",
      "17: , loss 1.42057, acc 0.95\n",
      "17: , loss 1.4713, acc 0.9\n",
      "17: , loss 1.37261, acc 1\n",
      "17: , loss 1.42188, acc 0.95\n",
      "17: , loss 1.3742, acc 1\n",
      "17: , loss 1.37245, acc 1\n",
      "17: , loss 1.3747, acc 1\n",
      "17: , loss 1.46715, acc 0.9\n",
      "17: , loss 1.46683, acc 0.9\n",
      "17: , loss 1.46717, acc 0.9\n",
      "17: , loss 1.41977, acc 0.95\n",
      "17: , loss 1.37296, acc 1\n",
      "17: , loss 1.37401, acc 1\n",
      "17: , loss 1.37269, acc 1\n",
      "17: , loss 1.49777, acc 0.85\n",
      "17: , loss 1.37359, acc 1\n",
      "17: , loss 1.51374, acc 0.85\n",
      "17: , loss 1.42128, acc 0.95\n",
      "17: , loss 1.37296, acc 1\n",
      "17: , loss 1.42058, acc 0.95\n",
      "17: , loss 1.47819, acc 0.9\n",
      "17: , loss 1.37242, acc 1\n",
      "17: , loss 1.37243, acc 1\n",
      "17: , loss 1.41957, acc 0.95\n",
      "17: , loss 1.3724, acc 1\n",
      "18: , loss 1.37273, acc 1\n",
      "18: , loss 1.41952, acc 0.95\n",
      "18: , loss 1.4201, acc 0.95\n",
      "18: , loss 1.37251, acc 1\n",
      "18: , loss 1.47102, acc 0.9\n",
      "18: , loss 1.41979, acc 0.95\n",
      "18: , loss 1.41996, acc 0.95\n",
      "18: , loss 1.4196, acc 0.95\n",
      "18: , loss 1.41961, acc 0.95\n",
      "18: , loss 1.37256, acc 1\n",
      "18: , loss 1.46664, acc 0.9\n",
      "18: , loss 1.42001, acc 0.95\n",
      "18: , loss 1.37236, acc 1\n",
      "18: , loss 1.37252, acc 1\n",
      "18: , loss 1.41961, acc 0.95\n",
      "18: , loss 1.41942, acc 0.95\n",
      "18: , loss 1.41942, acc 0.95\n",
      "18: , loss 1.4194, acc 0.95\n",
      "18: , loss 1.46633, acc 0.9\n",
      "18: , loss 1.37252, acc 1\n",
      "18: , loss 1.37236, acc 1\n",
      "18: , loss 1.46648, acc 0.9\n",
      "18: , loss 1.46949, acc 0.9\n",
      "18: , loss 1.4195, acc 0.95\n",
      "18: , loss 1.37235, acc 1\n",
      "18: , loss 1.41955, acc 0.95\n",
      "18: , loss 1.37273, acc 1\n",
      "18: , loss 1.41969, acc 0.95\n",
      "18: , loss 1.4193, acc 0.95\n",
      "18: , loss 1.41943, acc 0.95\n",
      "18: , loss 1.41962, acc 0.95\n",
      "18: , loss 1.37266, acc 1\n",
      "18: , loss 1.41928, acc 0.95\n",
      "18: , loss 1.46651, acc 0.9\n",
      "18: , loss 1.37306, acc 1\n",
      "18: , loss 1.4669, acc 0.9\n",
      "18: , loss 1.41969, acc 0.95\n",
      "18: , loss 1.41928, acc 0.95\n",
      "18: , loss 1.37232, acc 1\n",
      "18: , loss 1.51306, acc 0.85\n",
      "18: , loss 1.42239, acc 0.95\n",
      "18: , loss 1.41941, acc 0.95\n",
      "18: , loss 1.37257, acc 1\n",
      "19: , loss 1.3723, acc 1\n",
      "19: , loss 1.37237, acc 1\n",
      "19: , loss 1.41944, acc 0.95\n",
      "19: , loss 1.37268, acc 1\n",
      "19: , loss 1.37259, acc 1\n",
      "19: , loss 1.41927, acc 0.95\n",
      "19: , loss 1.46819, acc 0.9\n",
      "19: , loss 1.46653, acc 0.9\n",
      "19: , loss 1.41961, acc 0.95\n",
      "19: , loss 1.41922, acc 0.95\n",
      "19: , loss 1.41957, acc 0.95\n",
      "19: , loss 1.51327, acc 0.85\n",
      "19: , loss 1.51315, acc 0.85\n",
      "19: , loss 1.42029, acc 0.95\n",
      "19: , loss 1.37249, acc 1\n",
      "19: , loss 1.46647, acc 0.9\n",
      "19: , loss 1.37236, acc 1\n",
      "19: , loss 1.41942, acc 0.95\n",
      "19: , loss 1.51312, acc 0.85\n",
      "19: , loss 1.46612, acc 0.9\n",
      "19: , loss 1.37236, acc 1\n",
      "19: , loss 1.37256, acc 1\n",
      "19: , loss 1.4194, acc 0.95\n",
      "19: , loss 1.37233, acc 1\n",
      "19: , loss 1.37267, acc 1\n",
      "19: , loss 1.41944, acc 0.95\n",
      "19: , loss 1.37253, acc 1\n",
      "19: , loss 1.37259, acc 1\n",
      "19: , loss 1.37236, acc 1\n",
      "19: , loss 1.4196, acc 0.95\n",
      "19: , loss 1.37231, acc 1\n",
      "19: , loss 1.37242, acc 1\n",
      "19: , loss 1.41976, acc 0.95\n",
      "19: , loss 1.46659, acc 0.9\n",
      "19: , loss 1.41946, acc 0.95\n",
      "19: , loss 1.37244, acc 1\n",
      "19: , loss 1.41931, acc 0.95\n",
      "19: , loss 1.41982, acc 0.95\n",
      "19: , loss 1.46935, acc 0.9\n",
      "19: , loss 1.41942, acc 0.95\n",
      "19: , loss 1.37231, acc 1\n",
      "19: , loss 1.51634, acc 0.85\n",
      "19: , loss 1.37246, acc 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "测试程序 john.zhang 2016-11-28 已验证 程序可以运行\n",
    "\"\"\"\n",
    "# 数据集配置参数\n",
    "from DataSets_multi_task import datasets\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "# 文件信息\n",
    "file = 'datas_ace.txt'\n",
    "# 生成文件的存储位置\n",
    "store_path = \"ace_data_2016_12_02\"\n",
    "# batch_size的大小\n",
    "data_batch_size = 20\n",
    "# 句子的最大长度\n",
    "max_sequence_length = 25\n",
    "# 选取的上下文窗口的大小\n",
    "windows = 3\n",
    "# 数据集\n",
    "datas = datasets(file=file, store_path=store_path, batch_size=data_batch_size, max_sequence_length=max_sequence_length,\n",
    "                 windows=windows)\n",
    "# 神经网络模型的一些参数\n",
    "# 模型的最大长度\n",
    "sentence_length = max_sequence_length\n",
    "event_num_labels = datas.labels_event_size  # 事件类别数目\n",
    "role_num_labels = datas.labels_role_size  # 角色类别数目\n",
    "vocab_size = datas.words_size  # 训练集中词的数目\n",
    "word_embedding_size = 100  # 词嵌入维数\n",
    "pos_embedding_size = 10  # 位置嵌入维数\n",
    "filter_sizes = [3, 4, 5]  # 滤波器大小\n",
    "filter_num = 100  # 滤波器大小\n",
    "batch_size = None  # tensorflow中支持为定义长度的标记符合\n",
    "lr = 1e-3  # 学习率\n",
    "num_epochs = 20\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        # 模型文件\n",
    "        model = ace_cnn_model(sentence_length=sentence_length, \n",
    "                              event_num_labels=event_num_labels, \n",
    "                              role_num_labels=role_num_labels,\n",
    "                              vocab_size=vocab_size,\n",
    "                              word_embedding_size=word_embedding_size,\n",
    "                              pos_embedding_size=pos_embedding_size, \n",
    "                              filter_sizes=filter_sizes, \n",
    "                              filter_num=filter_num,\n",
    "                              batch_size=batch_size)\n",
    "        # 模型优化算法  两种方式：\n",
    "        # 方式一 采用两种优化器分别进行\n",
    "        # 方式二 将两种误差函数合并一起进行\n",
    "        optimizer_event = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars_event = optimizer_event.compute_gradients(model.loss_event) # 事件优化器\n",
    "        train_op_event = optimizer_event.apply_gradients(grads_and_vars_event)\n",
    "        \n",
    "        optimizer_role = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars_role = optimizer_role.compute_gradients(model.loss_role)  # 角色优化器\n",
    "        train_op_role = optimizer_role.apply_gradients(grads_and_vars_role)\n",
    "\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"ace_cnn_model_02\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables(), max_to_keep=100)  # 最大支持存储100个模型\n",
    "        # 初始化　变量\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        def train_step(input_x, input_event_y, input_role_y, input_t, input_c, input_t_pos, input_c_pos, dropout_keep_prob\n",
    "                       ,sentence_features,input_t_context,input_c_context,train_op,loss,accuracy,epoch):\n",
    "            feed_dict = {\n",
    "                model.input_x: input_x,\n",
    "                model.input_event_y: input_event_y,\n",
    "                model.input_role_y: input_role_y,\n",
    "                # model.input_t:input_t,\n",
    "                # model.input_c:input_c,\n",
    "                model.input_t_pos: input_t_pos,\n",
    "                model.input_c_pos: input_c_pos,\n",
    "                model.dropout_keep_prob: dropout_keep_prob,\n",
    "#                 model.sentence_features : sentence_features,\n",
    "                model.input_t_context:input_t_context,\n",
    "                model.input_c_context:input_c_context\n",
    "            }\n",
    "            _, loss, accuracy = sess.run(\n",
    "                [train_op, loss, accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: , loss {:g}, acc {:g}\".format(epoch, loss, accuracy))\n",
    "\n",
    "\n",
    "        # 测试阶段不需要计算梯度 也不需要进行权值更新 仅仅需要计算acc的值\n",
    "        def eval_step(input_x, input_event_y, input_role_y, input_t, input_c, input_t_pos, input_c_pos, dropout_keep_prob\n",
    "                      , sentence_features, input_t_context, input_c_context,train_op,loss,accuracy):\n",
    "            feed_dict = {\n",
    "                model.input_x: input_x,\n",
    "                model.input_event_y: input_event_y,\n",
    "                model.input_role_y: input_role_y,\n",
    "                # model.input_t:input_t,\n",
    "                # model.input_c:input_c,\n",
    "                model.input_t_pos: input_t_pos,\n",
    "                model.input_c_pos: input_c_pos,\n",
    "                model.dropout_keep_prob: dropout_keep_prob,\n",
    "#                 model.sentence_features: sentence_features,\n",
    "                model.input_t_context: input_t_context,\n",
    "                model.input_c_context: input_c_context\n",
    "            }\n",
    "            accuracy, predicts = sess.run([accuracy, predicts], feed_dict)\n",
    "            print (\"eval accuracy:{}\".format(accuracy))\n",
    "            return predicts\n",
    "\n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            for j in range(datas.instances_size // data_batch_size):\n",
    "                x, t, c, y_e, y_r, pos_c, pos_t, sentences_f, c_context, t_context, _ = datas.next_cnn_data()\n",
    "#                 print \", \".join(map(lambda t:datas.all_words[t]  , x[0])) \n",
    "                train_step(input_x=x, input_event_y=y_e, input_role_y=y_r,\n",
    "                           input_t=t, input_c=c, input_c_pos=pos_c, input_t_pos=pos_t,\n",
    "                           dropout_keep_prob=0.8, sentence_features=sentences_f, \n",
    "                           input_t_context= t_context,input_c_context = c_context,\n",
    "                           train_op = train_op_event, loss=model.loss_event,accuracy=model.accuracy_event,\n",
    "                           epoch = i\n",
    "                          )\n",
    "#         john.zhang 2016-12-04 将训练集当作测试集\n",
    "#         print \"----------------------------华丽的分割线-----------------------------------------\"\n",
    "#         x, t, c, y_e, y_r, pos_c, pos_t, sentences_f, c_context, t_context, _ = datas.eval_cnn_data()\n",
    "#         predicts = eval_step(input_x=x, input_y=y, input_t=t, input_c=c, input_c_pos=pos_c, input_t_pos=pos_t,\n",
    "#                   dropout_keep_prob=1.0, sentence_features=sentences_f, input_t_context= t_context,\n",
    "#                   input_c_context = c_context)\n",
    "\n",
    "#         # 输出测试结果\n",
    "#         for i in range(len(x)):\n",
    "#             print \"输入数据：{}\".format(\", \".join(map(lambda h: datas.all_words[h], x[i])))\n",
    "#             print \"触发词：{}\".format(\", \".join(map(lambda h: datas.all_words[h], t[i])))\n",
    "#             print \"候选词：{}\".format(\", \".join(map(lambda h: datas.all_words[h], c[i])))\n",
    "#             print \"预测事件类别:{}\".format(predicts[i])\n",
    "#             print \"----------------------------华丽的分割线-----------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}